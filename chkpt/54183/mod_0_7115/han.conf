model               = HANModel

## DATA PARAMS ##################################
item_id             = 54183
trait               = 0

# rand_seed           = 551493247
  
# text_pat            = {0}/text.tok
text_pat            = {0}/text.big.tok
min_cut             = 0.33 ## for subsampling large training files (default 1.0)

data_dir            = ../data
id_dir              = {}

#################################################

# no_shuffle
# keep_unk

test_pat            = {0}/test_ids{1}.txt
# test_pat            = {0}/temp_ids{1}.txt
test_cut            = 0.15
load_test
save_test

valid_pat           = {0}/valid_ids{1}.txt
valid_cut           = 0.2
load_valid
save_valid

chkpt_dir           = ../chkpt/{0}/mod{1}
vocab_file          = vocab_n250.txt
print_every         = 5
tokenize


## TRAINING PARAMS ##############################
batch_size          = 32
tensor_vol          = 50000
epochs              = 50
dropout             = 0.66
max_grad_norm       = 2.0

loss                = qwk

optimizer           = adam   # adam rmsprop adadelta adagrad adagradda    [ keras.rmsprop ]
learning_rate       = 0.001
# learning_rates       = '{ 1:0.001, 2:0.00075, 3:0.0005, 6:0.00025, 15:0.0002 }'

epoch_unfreeze_word = 50 # 6
epoch_unfreeze_emb  = 50 # 11
epoch_unfreeze_filt = 50 # 20

## EMBEDDING ####################################
embed_type          = word # word | char
embed_dir           = ../embeddings

## char embedding -------------------------------

char_embed_chkpt    = mod2_600-15
char_embed_size     = 15
kernel_widths       = '[1,2,3,4,5,6,7]'
kernel_features     = '[25,50,75,100,100,100,100]'

## word embedding -------------------------------

embed_dim           = 300

embed_path          = wikigiga.fasttext.sg.300d.txt
# embed_path          = glove.6B.{}d.txt # 50,100,200,300
# embed_path          = wiki.gensim.sg.300d.txt
# embed_path          = fasttext.sg.200d.m2.txt
# embed_path          = w2v_50d.txt
# embed_path          = conll.word2vec.sg.100d.txt

min_word_count      = 2

## MODEL PARAMS #################################

## DATA ##
split_sentences
sparse_words
trim_words

##                   wpad,  spad
pads            = "['post','pre']"

## RNN ##
rnn_new
rnn_dropout
rhn_highway_layers  = 3

# snt.lstm | snt.gru | lstm | gru | rhn  [ rwa | rwa_bn | rda ]

rnn_cells       = "['rhn','lstm']"
rnn_sizes       = '[300,  300]'
rnn_bis         = '[True, True]'

## ATTN ##

attn_sizes      = '[0, 0]' # [mean pool , mean pool]

attn_depths     = '[1, 1]'
attn_temps      = '[1, 1]'
attn_coef       = 0.01

###########################

model_std           = 0.1
attn_std            = 0.1
model_b             = 0.
attn_b              = 0.

ets18

