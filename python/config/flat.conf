model               = FlatModel

## DATA PARAMS ##################################
item_id             = 57186
trait               = 0

# rand_seed           = 448634400
  
# text_pat            = {0}/text.tok
text_pat            = {0}/text.big.tok
min_cut             = 0.2 ## for subsampling large training files (default 1.0)
  
data_dir            = /home/david/data/ats/ets_2018

#################################################

test_pat            = {0}/test_ids{1}.txt
# test_pat            = {0}/temp_ids{1}.txt
test_cut            = 0.1
load_test
# save_test

valid_pat           = {0}/valid_ids{1}.txt
valid_cut           = 0.1
# load_valid
# save_valid

chkpt_dir           = ../chkpt/{0}/mod{1}
vocab_file          = vocab_n250.txt
tokenize
print_every         = 5

## TRAINING PARAMS ##############################
batch_size          = 32

loss                = qwk
optimizer           = adam   # adam rmsprop adadelta adagrad adagradda    [ keras.rmsprop ]
learning_rate       = 0.001
# lr_decay            = 0.95
epochs              = 100
max_grad_norm       = 2.

dropout             = 0.5
# drop_sign           = -1

## EMBEDDING ####################################
embed_type          = word # word char
embed_dir           = ../embeddings

## char embedding -------------------------------

char_embed_chkpt    = mod2_600-15
char_embed_size     = 15
kernel_widths       = '[1,2,3,4,5,6,7]'
kernel_features     = '[25,50,75,100,100,100,100]'

## word embedding -------------------------------

embed_dim           = 100

# embed_path          = wikigiga.fasttext.sg.300d.txt
embed_path          = glove.6B.{}d.txt # 50,100,200,300
# embed_path          = wiki.gensim.sg.300d.txt
# embed_path          = fasttext.sg.200d.m2.txt
# embed_path          = w2v_50d.txt
# embed_path          = conll.word2vec.sg.100d.txt

min_word_count      = 3

## MODEL PARAMS #################################

## DATA ##
# split_sentences ## for HAN model only
sparse_words
trim_words
pads            = "['post']"

## RNN ##
rnn_new
rnn_dropout
rhn_highway_layers  = 3

# snt.lstm | snt.gru | lstm | gru | rhn  [ rwa | rwa_bn | rda ]
rnn_cells       = "['lstm']"
rnn_sizes       = '[ 300 ]'
rnn_bis         = '[ False ]'

## ATTN ##
attn_sizes      = '[ -2 ]'
attn_depths     = '[1]'
attn_temps      = '[1]'
attn_coef       = 0.01

# attn_vis

###########################

model_std           = 0.01
attn_std            = 0.01
model_b             = 0.
attn_b              = 0.

epoch_unfreeze_filt = 2
epoch_unfreeze_emb  = 3

ets18