model               = HANModel

## DATA PARAMS ##################################
item_id             = 63986
trait               = 0

# rand_seed           = 551493247
  
# text_pat            = {0}/text.tok
text_pat            = {0}/text.big.tok
min_cut             = 0.33 ## for subsampling large training files (default 1.0)
  
# data_dir            = /home/david/data/ats/ets_2018
data_dir            = ../data
id_dir              = {}

#################################################

# no_shuffle
# keep_unk

test_pat            = {0}/test_ids{1}.txt
# test_pat            = {0}/temp_ids{1}.txt
test_cut            = 0.15
load_test
save_test

valid_pat           = {0}/valid_ids{1}.txt
valid_cut           = 0.2
load_valid
save_valid

chkpt_dir           = ../chkpt/{0}/mod{1}
vocab_file          = vocab_n250.txt
print_every         = 5
tokenize
# spell_corr

## LOAD/SAVE PRE-TRAINED WEIGHTS
# load_chkpt_dir      = chkpt/pool1/mod{1}
# load_model
## save_model

## TRAINING PARAMS ##############################
batch_size          = 32
# tensor_vol          = 20000
# tensor_vol          = 30000
# tensor_vol          = 40000
tensor_vol          = 50000
# tensor_vol          = 75000
# tensor_vol          = 100000
# tensor_vol          = 150000
# tensor_vol          = 200000
# tensor_vol          = 300000
# tensor_vol          = 400000

epochs              = 50
dropout             = 0.5
# drop_sign           = -1
max_grad_norm       = 2.0

loss                = qwk
# loss                = mse

optimizer           = adam   # adam rmsprop adadelta adagrad adagradda    [ keras.rmsprop ]
learning_rate       = 0.001
# learning_rates       = '{ 1:0.001, 2:0.00075, 3:0.0005, 6:0.00025, 15:0.0002 }'

epoch_unfreeze_word = 50 # 6
epoch_unfreeze_emb  = 50 # 11
epoch_unfreeze_filt = 50 # 20

## EMBEDDING ####################################
embed_type          = word # word char

embed_dir           = ../embeddings

## char embedding -------------------------------

char_embed_chkpt    = mod2_600-15
char_embed_size     = 15
kernel_widths       = '[1,2,3,4,5,6,7]'
kernel_features     = '[25,50,75,100,100,100,100]'

## word embedding -------------------------------

embed_dim           = 300

# embed_path          = wikigiga.fasttext.sg.300d.txt
embed_path          = glove.6B.{}d.txt # 50,100,200,300
# embed_path          = wiki.gensim.sg.300d.txt
# embed_path          = fasttext.sg.200d.m2.txt
# embed_path          = w2v_50d.txt
# embed_path          = conll.word2vec.sg.100d.txt

min_word_count      = 2

## MODEL PARAMS #################################

## DATA ##
split_sentences ## for HAN model only
sparse_words
trim_words

##                   wpad,  spad
pads            = "['post','pre']"
# pads            = "['pre','post']"
# pads            = "['post','post']"

## RNN ##
rnn_new
rnn_dropout
rhn_highway_layers  = 3

# snt.lstm | snt.gru | lstm | gru | rhn  [ rwa | rwa_bn | rda ]

# rnn_cells       = "['lstm','lstm']"
# rnn_cells       = "['gru','gru']"
# rnn_cells       = "['rhn','gru']"
rnn_cells       = "['rhn','lstm']"

rnn_sizes       = '[300,  300]'
rnn_bis         = '[True, True]'

## RNN POOLING ##############################

# CODES for RNN POOLING:
# -2 = last state
# -1 = max pool (over all states)
#  0 = mean pool (unweighted mean)
# >0 = attn pool (weighted mean... weights learned by attention unit of given dim)
# 
# **note:
# - each element in sequence corresponds to a state
# - last state <=> state at last element
# - pooling over all states <=> pooling over all elements' states
# - 'attn_sizes' should probably be called 'pool_sizes' instead... only codes>0 correspond to attention....
# - see my ONLY stackoverflow question ever! (about attention): https://stackoverflow.com/questions/49522673

# attn_sizes      = '[0, 0]'    # [mean, mean]
attn_sizes      = '[-2, -2]'    # [last, last]
# attn_sizes      = '[-2, 0]'   # [last, mean]
# attn_sizes      = '[-2, 300]' # [last, attn]

# attn_sizes      = '[300, 0]'  # [attn, mean]
# attn_sizes      = '[300, -2]'
# attn_sizes      = '[300, 300]'
# attn_sizes      = '[600, 0]'
# attn_sizes      = '[600, 600]'

## these are relics of dead ends....
attn_depths     = '[1, 1]'
attn_temps      = '[1, 1]'
attn_coef       = 0.01

######################################

model_std           = 0.01
attn_std            = 0.01
model_b             = 0.
attn_b              = 0.

ets18
# new_split
