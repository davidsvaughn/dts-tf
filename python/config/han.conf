model               = HANModel
batcher             = ResponseBatcher

## TENSORBOARD  #################################
# tensorboard --logdir=mod --port 6006 --debugger_port 6064

## DATA PARAMS ##################################
## 57186 56356
## 55381 61711 61993 61378 56525
## 61851 54183 54703 54151
item_id             = 54183 ## 55427 61851 55381 pool1 63986 56356 61851 54183 55427
trait               = 0

# rand_seed           = 551493247
  
# text_pat            = {0}/text.tok
text_pat            = {0}/text.big.tok
min_cut             = 0.33 ## for subsampling large training files (default 1.0)
  
data_dir            = /home/david/data/ats/ets_2018
# data_dir            = ../data
id_dir              = {}

#################################################

# no_shuffle
# keep_unk

test_pat            = {0}/test_ids{1}.txt
# test_pat            = {0}/temp_ids{1}.txt
test_cut            = 0.15
load_test
save_test

valid_pat           = {0}/valid_ids{1}.txt
valid_cut           = 0.2
load_valid
save_valid

chkpt_dir           = ../chkpt/{0}/mod{1}
vocab_file          = vocab_n250.txt
print_every         = 5
tokenize
# spell_corr

## LOAD/SAVE PRE-TRAINED WEIGHTS
# load_chkpt_dir      = chkpt/pool1/mod{1}
# load_model
## save_model

## TRAINING PARAMS ##############################
batch_size          = 32
# tensor_vol          = 20000
# tensor_vol          = 30000
# tensor_vol          = 40000
tensor_vol          = 50000
# tensor_vol          = 75000
# tensor_vol          = 100000
# tensor_vol          = 150000
# tensor_vol          = 200000
# tensor_vol          = 300000
# tensor_vol          = 400000

epochs              = 50
dropout             = 0.6
# drop_sign           = -1
max_grad_norm       = 2.0

loss                = qwk
# loss                = mse

optimizer           = adam   # adam rmsprop adadelta adagrad adagradda    [ keras.rmsprop ]
learning_rate       = 0.001
# learning_rates       = '{ 1:0.001, 2:0.00075, 3:0.0005, 6:0.00025, 15:0.0002 }'

epoch_unfreeze_word = 50 # 6
epoch_unfreeze_emb  = 50 # 11
epoch_unfreeze_filt = 50 # 20

## EMBEDDING ####################################
embed_type          = word # word char

## char embedding -------------------------------
kernel_widths       = '[1,2,3,4,5,6,7]'

char_embed_size     = 15
kernel_features     = '[25,50,75,100,100,100,100]'
char_embed_chkpt    = /home/david/code/python/dts-tf/lm_char/mod2_600-15

# char_embed_size     = 20
# kernel_features     = '[50,100,150,200,200,200,200]'
# char_embed_chkpt    = /home/david/code/python/dts-tf/lm_char/mod1_650-20

# char_embed_chkpt    = /home/david/code/python/tf-lstm-char-cnn/chkpt/mod2_600-15

## word embedding -------------------------------

embed_dim           = 300

# embed_path          = ../embeddings/word/glove.6B.{}d.txt # 50,100,200,300
embed_path          = ../embeddings/word/wikigiga.fasttext.sg.300d.txt
# embed_path          = ../embeddings/word/fasttext.sg.200d.m2.txt
# embed_path          = ../embeddings/word/w2v_50d.txt
# embed_path          = ../embeddings/word/conll.word2vec.sg.100d.txt


# embed_path          = /home/david/data/embed/glove.6B.{}d.txt # 50,100,200,300
# embed_path          = /home/david/data/embed/wikigiga.fasttext.sg.300d.txt
# embed_path          = /home/david/data/embed/wiki.gensim.sg.300d.txt

# embed_path          = /home/david/data/embed/fasttext.sg.200d.m1.txt
# embed_path          = /home/david/data/embed/fasttext.sg.200d.m2.txt
# embed_dim           = 200

# embed_path          = /home/david/data/embed/w2v_50d.txt
# embed_path          = /home/david/data/embed/conll.word2vec.sg.100d.txt

min_word_count      = 2

## MODEL PARAMS #################################

## DATA ##
split_sentences
sparse_words
trim_words

##                   wpad,  spad
pads            = "['post','pre']"
# pads            = "['pre','post']"
# pads            = "['post','post']"

## RNN ##
rnn_new
rnn_dropout
rhn_highway_layers  = 3

# snt.lstm | snt.gru | lstm | gru | rhn  [ rwa | rwa_bn | rda ]

# rnn_cells       = "['lstm','lstm']"
# rnn_cells       = "['gru','gru']"
# rnn_cells       = "['rhn','gru']"
rnn_cells       = "['rhn','lstm']"

rnn_sizes       = '[300,  300]'
rnn_bis         = '[True, False]'

## ATTN ##
# attn_sizes      = '[300, 0]'
# attn_sizes      = '[300, -2]'
# attn_sizes      = '[300, 300]'
# attn_sizes      = '[600, 0]'
# attn_sizes      = '[600, 600]'

# attn_sizes      = '[0, 0]'
# attn_sizes      = '[-2, -2]' # [last, last]
attn_sizes      = '[-2, 300]' # [last, last]

attn_depths     = '[1, 1]'
attn_temps      = '[1, 1]'
attn_coef       = 0.01

# attn_vis

######################################
## BEST: 57186
# rnn_cells       = "['lstm','lstm']"
# rnn_sizes       = '[300,  100]'
# rnn_bis         = '[True, False]'
# attn_sizes      = '[300, 0]'
# pads            = "['pre','post']"

###########################

model_std           = 0.1
attn_std            = 0.1
model_b             = 0.
attn_b              = 0.

ets18
# new_split
